{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import errno\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compasdataset(df):\n",
    "    df = df[['age', 'c_charge_degree', 'race', 'age_cat', 'score_text',\n",
    "                 'sex', 'priors_count', 'days_b_screening_arrest', 'decile_score',\n",
    "                 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']]\n",
    "\n",
    "    # Indices of data samples to keep\n",
    "    ix = df['days_b_screening_arrest'] <= 30\n",
    "    ix = (df['days_b_screening_arrest'] >= -30) & ix\n",
    "    ix = (df['is_recid'] != -1) & ix\n",
    "    ix = (df['c_charge_degree'] != \"O\") & ix\n",
    "    ix = (df['score_text'] != 'N/A') & ix\n",
    "    df = df.loc[ix,:]\n",
    "    df['length_of_stay'] = (pd.to_datetime(df['c_jail_out']) - pd.to_datetime(df['c_jail_in'])).apply(lambda x: x.days)\n",
    "\n",
    "    # Restrict races to African-American and Caucasian\n",
    "    df = df.loc[~df['race'].isin(['Native American','Hispanic','Asian','Other']),:]\n",
    "\n",
    "    df = df[['sex','race','age_cat','c_charge_degree','score_text','priors_count','is_recid', 'two_year_recid','length_of_stay']]\n",
    "\n",
    "    df['priors_count'] = df['priors_count'].apply(lambda x: 0 if x <= 0 else ('1 to 3' if 1 <= x <= 3 else 'More than 3'))\n",
    "    df['length_of_stay'] = df['length_of_stay'].apply(lambda x: '<week' if x <= 7 else ('<3months' if 8 < x <= 93 else '>3months'))\n",
    "    df['score_text'] = df['score_text'].apply(lambda x: 'MediumHigh' if (x == 'High')| (x == 'Medium') else x)\n",
    "    df['age_cat'] = df['age_cat'].apply(lambda x: '25 to 45' if x == '25 - 45' else x)\n",
    "\n",
    "    df['sex'] = df['sex'].replace({'Female': 1.0, 'Male': 0.0})\n",
    "    df['race'] = df['race'].apply(lambda x: 1.0 if x == 'Caucasian' else 0.0)\n",
    "\n",
    "    df = df[['two_year_recid', 'sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree']]\n",
    "\n",
    "    protected_attributes = ['sex', 'race']\n",
    "    label_name = 'two_year_recid'\n",
    "    categorical_features = ['age_cat', 'priors_count', 'c_charge_degree']\n",
    "    features = categorical_features + [label_name] + protected_attributes\n",
    "\n",
    "    # privileged classes\n",
    "    privileged_classes = {\"sex\": [1.0], \"race\": [1.0]}\n",
    "\n",
    "    # protected attribute maps\n",
    "    protected_attribute_map = {\"sex\": {0.0: 'Male', 1.0: 'Female'},\n",
    "                                \"race\": {1.0: 'Caucasian', 0.0: 'Not Caucasian'}}\n",
    "\n",
    "\n",
    "    data = StandardDataset(df, label_name, favorable_classes=[0],\n",
    "                           protected_attribute_names=protected_attributes,\n",
    "                           privileged_classes=[privileged_classes[x] for x in protected_attributes],\n",
    "                           categorical_features=categorical_features,\n",
    "                           features_to_keep=features,\n",
    "                           metadata={'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "                                     'protected_attribute_maps': [protected_attribute_map[x] for x in protected_attributes]})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_germandataset(df):\n",
    "    def group_credit_hist(x):\n",
    "        if x in ['no credits taken/ all credits paid back duly', 'all credits at this bank paid back duly', 'existing credits paid back duly till now']:\n",
    "            return 'None/Paid'\n",
    "        elif x == 'delay in paying off in the past':\n",
    "            return 'Delay'\n",
    "        elif x == 'critical account/ other credits existing (not at this bank)':\n",
    "            return 'Other'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "    def group_employ(x):\n",
    "        if x == 'unemployed':\n",
    "            return 'Unemployed'\n",
    "        elif x in ['... < 1 year ', '1 <= ... < 4 years']:\n",
    "            return '1-4 years'\n",
    "        elif x in ['4 <= ... < 7 years', '.. >= 7 years']:\n",
    "            return '4+ years'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "    def group_savings(x):\n",
    "        if x in ['... < 100 DM', '100 <= ... < 500 DM']:\n",
    "            return '<500'\n",
    "        elif x in ['500 <= ... < 1000 DM ', '.. >= 1000 DM ']:\n",
    "            return '500+'\n",
    "        elif x == 'unknown/ no savings account':\n",
    "            return 'Unknown/None'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "    def group_status(x):\n",
    "        if x in ['< 0 DM', '0 <= ... < 200 DM']:\n",
    "            return '<200'\n",
    "        elif x in ['>= 200 DM / salary assignments for at least 1 year']:\n",
    "            return '200+'\n",
    "        elif x == 'no checking account':\n",
    "            return 'None'\n",
    "        else:\n",
    "            return 'NA'\n",
    "\n",
    "    status_map = {'male : divorced/separated': 1.0,\n",
    "                'male : single': 1.0,\n",
    "                'male : married/widowed': 1.0,\n",
    "                'female : divorced/separated/married': 0.0,\n",
    "                'female : single': 0.0}\n",
    "\n",
    "    df['personal_status_sex'] = df['personal_status_sex'].replace(status_map)\n",
    "    df['credit_history'] = df['credit_history'].apply(lambda x: group_credit_hist(x))\n",
    "    df['savings'] = df['savings'].apply(lambda x: group_savings(x))\n",
    "    df['present_emp_since'] = df['present_emp_since'].apply(lambda x: group_employ(x))\n",
    "    df['age'] = df['age'].apply(lambda x: np.float(x >= 25))\n",
    "    df['account_check_status'] = df['account_check_status'].apply(lambda x: group_status(x))\n",
    "\n",
    "    df = df.rename(columns = {'default': 'credit', 'present_emp_since': 'employment', 'account_check_status': 'status', 'personal_status_sex': 'sex'})\n",
    "\n",
    "    protected_attribute = ['sex', 'age']\n",
    "    label_name = 'credit'\n",
    "    categorical_features = ['credit_history', 'savings', 'employment']\n",
    "    features = categorical_features + [label_name] + protected_attribute\n",
    "\n",
    "    privileged_class = {'sex': [1.0], 'age': [1.0]}\n",
    "\n",
    "    protected_attribute_map = {\"sex\": {1.0: 'male', 0.0: 'female'},\n",
    "                            \"age\": {1.0: 'old', 0.0: 'young'}}\n",
    "\n",
    "    data = StandardDataset(df, label_name, favorable_classes=[1],\n",
    "                            protected_attribute_names=protected_attribute,\n",
    "                            privileged_classes=[privileged_class[x] for x in protected_attribute],\n",
    "                            categorical_features=categorical_features,\n",
    "                            features_to_keep=features,\n",
    "                            metadata={'label_maps': [{1.0: 'Good Credit', 2.0: 'Bad Credit'}],\n",
    "                                    'protected_attribute_maps': [protected_attribute_map[x] for x in protected_attribute]})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Reweighing ##############\n",
    "\n",
    "def reweighing_data(train, unprivileged_group, privileged_group):\n",
    "    RW = Reweighing(unprivileged_groups=unprivileged_group, privileged_groups=privileged_group)\n",
    "    RW.fit(train)\n",
    "    train_transformed = RW.transform(train)\n",
    "    \n",
    "    train_set = train_transformed\n",
    "\n",
    "    # change weights to whole numbers\n",
    "    for i in range(train_transformed.instance_weights.size):\n",
    "        train_transformed.instance_weights[i] = (round(train_transformed.instance_weights[i] / 0.1) * 0.1) * 10\n",
    "        weights = copy.deepcopy(train_transformed.instance_weights)\n",
    "\n",
    "    # change train_transformed.features and train_transformed.labels and train_transformed.protected_attributes according to the weights of each instance\n",
    "    for i in range(train_transformed.features.shape[0]):\n",
    "        row = copy.deepcopy(train_transformed.features[i])\n",
    "        row_label = copy.deepcopy(train_transformed.labels[i])\n",
    "        row_protected_attributes = copy.deepcopy(train_transformed.protected_attributes[i])\n",
    "        row_protected_attributes.resize(1,2)\n",
    "        row.resize(1,train_transformed.features.shape[1])\n",
    "        row_label.resize(1,1)\n",
    "        weight = int(weights[i])\n",
    "        for j in range(weight-1):\n",
    "            train_transformed.features = np.concatenate((train_transformed.features,row))\n",
    "            train_transformed.labels = np.concatenate((train_transformed.labels,row_label))\n",
    "            train_transformed.protected_attributes = np.concatenate((train_transformed.protected_attributes,row_protected_attributes))\n",
    "\n",
    "    # change the train_transformed to a numpy array of ones to match number of rows in features\n",
    "    train_transformed.instance_weights = np.ones(train_transformed.features.shape[0])\n",
    "\n",
    "    print(\"reweighing complete\\n\")\n",
    "\n",
    "    return train_set, train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_rates(input_data, output_data, attribute_name, privileged=None, unprivileged=None, favourable=None, unfavourable=None):\n",
    "\n",
    "    index_attribute = input_data.feature_names.index(attribute_name)\n",
    "    privileged = float(privileged)\n",
    "    unprivileged = float(unprivileged)\n",
    "    \n",
    "    input_priv = input_data.labels[np.where(input_data.features[:,index_attribute] == privileged)]\n",
    "    output_priv = output_data.labels[np.where(output_data.features[:,index_attribute] == privileged)]\n",
    "    priv_labels = np.concatenate((input_priv, output_priv), axis=1)\n",
    "    \n",
    "    input_unpriv = input_data.labels[np.where(input_data.features[:,index_attribute] == unprivileged)]\n",
    "    output_unpriv = output_data.labels[np.where(output_data.features[:,index_attribute] == unprivileged)]\n",
    "    unpriv_labels = np.concatenate((input_unpriv, output_unpriv), axis=1)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(priv_labels.shape[0]):\n",
    "        input_label = priv_labels[i][0]\n",
    "        output_label = priv_labels[i][1]\n",
    "        if input_label == output_label:\n",
    "            if input_label == unfavourable:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                tp = tp + 1\n",
    "        else:\n",
    "            if input_label == favourable and output_label == unfavourable:\n",
    "                fn = fn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "    \n",
    "    rates_privileged = [tp,fp,tn,fn]\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(unpriv_labels.shape[0]):\n",
    "        input_label = unpriv_labels[i][0]\n",
    "        output_label = unpriv_labels[i][1]\n",
    "        if input_label == output_label:\n",
    "            if input_label == unfavourable:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                tp = tp + 1\n",
    "        else:\n",
    "            if input_label == favourable and output_label == unfavourable:\n",
    "                fn = fn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "                \n",
    "    rates_unprivileged = [tp,fp,tn,fn]  \n",
    "    \n",
    "    rates_list = [rates_privileged, rates_unprivileged]\n",
    "    \n",
    "    return rates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_opp_diff(input_data, output_data, attribute_name, privileged, unprivileged, favourable, unfavourable):\n",
    "    rates_both = output_rates(input_data, output_data, attribute_name, privileged, unprivileged, favourable, unfavourable)\n",
    "    \n",
    "    # [tp, fp, tn, fn]\n",
    "    outcome_privileged = rates_both[0]\n",
    "    outcome_unprivileged = rates_both[1]\n",
    "    \n",
    "    # true positive rate = tp / (tp + fn)\n",
    "    tpr_privileged = outcome_privileged[0] / (outcome_privileged[0] + outcome_privileged[3])\n",
    "    tpr_unprivileged = outcome_unprivileged[0] / (outcome_unprivileged[0] + outcome_unprivileged[3])\n",
    "\n",
    "    fairness = tpr_unprivileged - tpr_privileged\n",
    "    \n",
    "    return fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_odds_diff(input_data, output_data, attribute_name, privileged, unprivileged, favourable, unfavourable):\n",
    "    rates_both = output_rates(input_data, output_data, attribute_name, privileged, unprivileged, favourable, unfavourable)\n",
    "    \n",
    "    # [tp, fp, tn, fn]\n",
    "    outcome_privileged = rates_both[0]\n",
    "    outcome_unprivileged = rates_both[1]\n",
    "    \n",
    "    # true positive rate = tp / (tp + fn)\n",
    "    tpr_privileged = outcome_privileged[0] / (outcome_privileged[0] + outcome_privileged[3])\n",
    "    tpr_unprivileged = outcome_unprivileged[0] / (outcome_unprivileged[0] + outcome_unprivileged[3])\n",
    "\n",
    "    # false positive rate = fp / (fp + tn)\n",
    "    fpr_privileged = outcome_privileged[1] / (outcome_privileged[1] + outcome_privileged[2])\n",
    "    fpr_unprivileged = outcome_unprivileged[1] / (outcome_unprivileged[1] + outcome_unprivileged[2])\n",
    "    \n",
    "    fpr_diff = fpr_unprivileged - fpr_privileged\n",
    "    tpr_diff = tpr_unprivileged - tpr_unprivileged\n",
    "    \n",
    "    fairness = (fpr_diff + tpr_diff) * 0.5\n",
    "    \n",
    "    return fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(train, test, unprivileged_groups, privileged_groups):\n",
    "    \n",
    "    ################## adversarial debiasing #################\n",
    "\n",
    "    sess = tf.Session()\n",
    "    debiased_model_reweighing = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                                     unprivileged_groups = unprivileged_groups,\n",
    "                                                     scope_name='debiased_classifier', debias=True, sess=sess)\n",
    "    debiased_model_reweighing.fit(train)\n",
    "    dataset_debiasing_test_reweighing = debiased_model_reweighing.predict(test)\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_debiasing_test_reweighing,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_debiasing_test_reweighing,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "\n",
    "    accuracy_adversarial = accuracy_score(y_true = test.labels, y_pred = dataset_debiasing_test_reweighing.labels)\n",
    "    \n",
    "    metrics_adversarial = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### prejudice remover #####################\n",
    "    prejudice_model_reweighing = PrejudiceRemover(eta=100, sensitive_attr='sex')\n",
    "    prejudice_model_reweighing.fit(train)\n",
    "    dataset_prejudice_test_reweighing = prejudice_model_reweighing.predict(test)\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_prejudice_test_reweighing,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_prejudice_test_reweighing,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_prejudice = accuracy_score(y_true=test.labels, y_pred=dataset_prejudice_test_reweighing.labels)\n",
    "    \n",
    "    equal_opportunity_difference = equal_opp_diff(test, dataset_debiasing_test_reweighing,\n",
    "                                                  'sex', privileged=1, unprivileged=0, favourable=1, unfavourable=0)\n",
    "    \n",
    "    average_odds_difference = avg_odds_diff(test, dataset_debiasing_test_reweighing,\n",
    "                                                  'sex', privileged=1, unprivileged=0, favourable=1, unfavourable=0)\n",
    "    \n",
    "    metrics_prejudice = [metric_test.mean_difference(), acc_test.disparate_impact(), equal_opportunity_difference, average_odds_difference, acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### normal neural net #####################\n",
    "    sess = tf.Session()\n",
    "    neural_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                        unprivileged_groups = unprivileged_groups,\n",
    "                                        scope_name='debiased_classifier', debias=False, sess=sess)\n",
    "    neural_model.fit(train)\n",
    "    dataset_neural_test = neural_model.predict(test)\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_neural_test,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_neural_test,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_nondebiasing = accuracy_score(y_true=test.labels, y_pred=dataset_neural_test.labels)\n",
    "\n",
    "    metrics_nondebiasing = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### ensemble #####################\n",
    "    pred_labels_test = []\n",
    "    for i in range(0, len(test.features)):\n",
    "        arr_test = mode([dataset_debiasing_test_reweighing.labels[i], dataset_prejudice_test_reweighing.labels[i], dataset_neural_test.labels[i]])\n",
    "        pred_labels_test.append(arr_test[0][0])\n",
    "        dataset_ensemble_test = test.copy()\n",
    "        dataset_ensemble_test.labels = np.array(pred_labels_test)\n",
    "\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_ensemble_test,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_ensemble_test,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_ensemble = accuracy_score(y_true=test.labels, y_pred=dataset_ensemble_test.labels)\n",
    "    \n",
    "    metrics_ensemble = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "    accuracy_scores = [accuracy_adversarial, accuracy_prejudice, accuracy_nondebiasing, accuracy_ensemble]\n",
    "    fairness_metrics = [metrics_adversarial, metrics_prejudice, metrics_nondebiasing, metrics_ensemble]\n",
    "    \n",
    "    return accuracy_scores, fairness_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with Compas data set\n",
      "reweighing complete\n",
      "\n",
      "run = 1\n",
      "run = 2\n",
      "run = 3\n",
      "run = 4\n",
      "run = 5\n",
      "run = 6\n",
      "run = 7\n",
      "run = 8\n",
      "run = 9\n",
      "prediction completed\n"
     ]
    }
   ],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "print(\"Classification with Compas data set\")\n",
    "df = pd.read_csv('dataset/compas-scores-two-years.csv')\n",
    "dataset_orig = preprocess_compasdataset(df)\n",
    "\n",
    "train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "train_transformed, dataset_transf_train = reweighing_data(train, unprivileged_groups, privileged_groups)\n",
    "\n",
    "for i in range(0,9):\n",
    "    print('run =', i+1)\n",
    "    \n",
    "    # with reweighing\n",
    "    accuracy_reweigh, fairness_metrics_reweigh =  make_prediction(dataset_transf_train, test, unprivileged_groups, privileged_groups)\n",
    "    metrics_adversarial_reweigh = fairness_metrics_reweigh[0]\n",
    "    metrics_prejudice_reweigh = fairness_metrics_reweigh[1]\n",
    "    metrics_nondebiasing_reweigh = fairness_metrics_reweigh[2]\n",
    "    metrics_ensemble_reweigh = fairness_metrics_reweigh[3]\n",
    "\n",
    "    # Without reweighing\n",
    "    accuracy, fairness_metrics =  make_prediction(train_transformed, test, unprivileged_groups, privileged_groups)\n",
    "    metrics_adversarial = fairness_metrics[0]\n",
    "    metrics_prejudice = fairness_metrics[1]\n",
    "    metrics_nondebiasing = fairness_metrics[2]\n",
    "    metrics_ensemble = fairness_metrics[3]\n",
    "\n",
    "print('\\nprediction completed')\n",
    "\n",
    "mean_difference = [metrics_adversarial_reweigh[0], metrics_prejudice_reweigh[0], metrics_nondebiasing_reweigh[0], metrics_ensemble_reweigh[0]]\n",
    "disparate_impact = [metrics_adversarial_reweigh[1], metrics_prejudice_reweigh[1], metrics_nondebiasing_reweigh[1], metrics_ensemble_reweigh[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial_reweigh[2], metrics_prejudice_reweigh[2], metrics_nondebiasing_reweigh[2], metrics_ensemble_reweigh[2]]\n",
    "average_odds_difference = [metrics_adversarial_reweigh[3], metrics_prejudice_reweigh[3], metrics_nondebiasing_reweigh[3], metrics_ensemble_reweigh[3]]\n",
    "theil_index = [metrics_adversarial_reweigh[4], metrics_prejudice_reweigh[4], metrics_nondebiasing_reweigh[4], metrics_ensemble_reweigh[4]]\n",
    "\n",
    "compas_reweighted = [accuracy_reweigh, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "\n",
    "\n",
    "mean_difference = [metrics_adversarial[0], metrics_prejudice[0], metrics_nondebiasing[0], metrics_ensemble[0]]\n",
    "disparate_impact = [metrics_adversarial[1], metrics_prejudice[1], metrics_nondebiasing[1], metrics_ensemble[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial[2], metrics_prejudice[2], metrics_nondebiasing[2], metrics_ensemble[2]]\n",
    "average_odds_difference = [metrics_adversarial[3], metrics_prejudice[3], metrics_nondebiasing[3], metrics_ensemble[3]]\n",
    "theil_index = [metrics_adversarial[4], metrics_prejudice[4], metrics_nondebiasing[4], metrics_ensemble[4]]\n",
    "\n",
    "compas_nonreweighted = [accuracy, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "\n",
    "print('compiled all metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with German data set\n",
      "reweighing complete\n",
      "\n",
      "run = 1\n",
      "run = 2\n",
      "run = 3\n",
      "run = 4\n",
      "run = 5\n",
      "run = 6\n",
      "run = 7\n",
      "run = 8\n",
      "run = 9\n",
      "prediction completed\n"
     ]
    }
   ],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "print(\"Classification with German data set\")\n",
    "df = pd.read_csv('dataset/german_credit.csv')\n",
    "dataset_orig = preprocess_germandataset(df)\n",
    "\n",
    "train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "train_transformed, dataset_transf_train = reweighing_data(train, unprivileged_groups, privileged_groups)\n",
    "\n",
    "for i in range(0,9):\n",
    "    print('run =', i+1)\n",
    "    \n",
    "    # With reweighing\n",
    "    accuracy_reweigh, fairness_metrics_reweigh =  make_prediction(dataset_transf_train, test, unprivileged_groups, privileged_groups)\n",
    "    metrics_adversarial_reweigh = fairness_metrics_reweigh[0]\n",
    "    metrics_prejudice_reweigh = fairness_metrics_reweigh[1]\n",
    "    metrics_nondebiasing_reweigh = fairness_metrics_reweigh[2]\n",
    "    metrics_ensemble_reweigh = fairness_metrics_reweigh[3]\n",
    "\n",
    "    # Without reweighing\n",
    "    accuracy, fairness_metrics =  make_prediction(train_transformed, test, unprivileged_groups, privileged_groups)\n",
    "    metrics_adversarial = fairness_metrics[0]\n",
    "    metrics_prejudice = fairness_metrics[1]\n",
    "    metrics_nondebiasing = fairness_metrics[2]\n",
    "    metrics_ensemble = fairness_metrics[3]\n",
    "\n",
    "print('\\nprediction completed')\n",
    "\n",
    "mean_difference = [metrics_adversarial_reweigh[0], metrics_prejudice_reweigh[0], metrics_nondebiasing_reweigh[0], metrics_ensemble_reweigh[0]]\n",
    "disparate_impact = [metrics_adversarial_reweigh[1], metrics_prejudice_reweigh[1], metrics_nondebiasing_reweigh[1], metrics_ensemble_reweigh[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial_reweigh[2], metrics_prejudice_reweigh[2], metrics_nondebiasing_reweigh[2], metrics_ensemble_reweigh[2]]\n",
    "average_odds_difference = [metrics_adversarial_reweigh[3], metrics_prejudice_reweigh[3], metrics_nondebiasing_reweigh[3], metrics_ensemble_reweigh[3]]\n",
    "theil_index = [metrics_adversarial_reweigh[4], metrics_prejudice_reweigh[4], metrics_nondebiasing_reweigh[4], metrics_ensemble_reweigh[4]]\n",
    "\n",
    "german_reweighted = [accuracy_reweigh, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "\n",
    "\n",
    "mean_difference = [metrics_adversarial[0], metrics_prejudice[0], metrics_nondebiasing[0], metrics_ensemble[0]]\n",
    "disparate_impact = [metrics_adversarial[1], metrics_prejudice[1], metrics_nondebiasing[1], metrics_ensemble[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial[2], metrics_prejudice[2], metrics_nondebiasing[2], metrics_ensemble[2]]\n",
    "average_odds_difference = [metrics_adversarial[3], metrics_prejudice[3], metrics_nondebiasing[3], metrics_ensemble[3]]\n",
    "theil_index = [metrics_adversarial[4], metrics_prejudice[4], metrics_nondebiasing[4], metrics_ensemble[4]]\n",
    "\n",
    "german_nonreweighted = [accuracy, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "\n",
    "print('compiled all metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Classification with Compas data set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics with reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.656566   0.659091     0.660985  0.659722\n",
      "Mean difference                  0.024422  -0.049253    -0.015826  0.002750\n",
      "Disparate impact                 1.045075   0.909609     0.972809  1.004880\n",
      "Equal Opportunity Difference     0.101556  -0.021694     0.055894  0.074159\n",
      "Average Odds Difference          0.061625  -0.050778     0.024371  0.043119\n",
      "Theil Index                      0.233238   0.261755     0.225764  0.228675\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics without reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.663510   0.659091     0.659722  0.663510\n",
      "Mean difference                 -0.046786  -0.049253     0.002750 -0.046786\n",
      "Disparate impact                 0.923678   0.909609     1.004880  0.923678\n",
      "Equal Opportunity Difference     0.023931   0.035998     0.074159  0.023931\n",
      "Average Odds Difference         -0.006034  -0.011965     0.043119 -0.006034\n",
      "Theil Index                      0.220645   0.261755     0.228675  0.220645\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Classification with German data set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics with reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.700000   0.723333     0.686667  0.696667\n",
      "Mean difference                 -0.052468  -0.004854    -0.025408 -0.057323\n",
      "Disparate impact                 0.168576   0.000000     0.626140  0.156535\n",
      "Equal Opportunity Difference    -0.055556  -0.055556    -0.055556 -0.055556\n",
      "Average Odds Difference         -0.053097  -0.025319    -0.033659 -0.056386\n",
      "Theil Index                      0.322775   0.321041     0.328063  0.324141\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics without reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.710000   0.723333     0.700000  0.710000\n",
      "Mean difference                 -0.028197  -0.004854    -0.052468 -0.028197\n",
      "Disparate impact                 0.273936   0.000000     0.168576  0.273936\n",
      "Equal Opportunity Difference    -0.037037  -0.037037    -0.055556 -0.037037\n",
      "Average Odds Difference         -0.030680  -0.012161    -0.053097 -0.030680\n",
      "Theil Index                      0.321582   0.321041     0.322775  0.321582\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"### Classification with Compas data set\"))\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics with reweighing\"))\n",
    "\n",
    "columns = ['Adversarial', 'Prejudice', 'Nondebiased', 'Ensemble']\n",
    "index = ['Accuracy', 'Mean difference', 'Disparate impact', 'Equal Opportunity Difference', 'Average Odds Difference', 'Theil Index']\n",
    "\n",
    "df_compas_reweigh = pd.DataFrame(compas_reweighted, index = index, columns=columns)\n",
    "print(df_compas_reweigh)\n",
    "\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics without reweighing\"))\n",
    "df_compas_nonreweigh = pd.DataFrame(compas_nonreweighted, index = index, columns=columns)\n",
    "print(df_compas_nonreweigh)\n",
    "\n",
    "display(Markdown(\"### Classification with German data set\"))\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics with reweighing\"))\n",
    "\n",
    "df_german_reweigh = pd.DataFrame(german_reweighted, index = index, columns=columns)\n",
    "print(df_german_reweigh)\n",
    "\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics without reweighing\"))\n",
    "df_german_nonreweigh = pd.DataFrame(german_nonreweighted, index = index, columns=columns)\n",
    "print(df_german_nonreweigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output result to csv\n",
    "\n",
    "df_compas_reweigh.to_csv(\"results/df_compas_reweigh.csv\", encoding='utf-8')\n",
    "df_compas_nonreweigh.to_csv(\"results/df_compas_nonreweigh.csv\", encoding='utf-8')\n",
    "df_german_reweigh.to_csv(\"results/df_german_reweigh.csv\", encoding='utf-8')\n",
    "df_german_nonreweigh.to_csv(\"results/df_german_nonreweigh.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
