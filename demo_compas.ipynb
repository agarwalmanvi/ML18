{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import errno\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compasdataset(df):\n",
    "    df = df[['age', 'c_charge_degree', 'race', 'age_cat', 'score_text',\n",
    "                 'sex', 'priors_count', 'days_b_screening_arrest', 'decile_score',\n",
    "                 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']]\n",
    "\n",
    "    # Indices of data samples to keep\n",
    "    ix = df['days_b_screening_arrest'] <= 30\n",
    "    ix = (df['days_b_screening_arrest'] >= -30) & ix\n",
    "    ix = (df['is_recid'] != -1) & ix\n",
    "    ix = (df['c_charge_degree'] != \"O\") & ix\n",
    "    ix = (df['score_text'] != 'N/A') & ix\n",
    "    df = df.loc[ix,:]\n",
    "    df['length_of_stay'] = (pd.to_datetime(df['c_jail_out']) - pd.to_datetime(df['c_jail_in'])).apply(lambda x: x.days)\n",
    "\n",
    "    # Restrict races to African-American and Caucasian\n",
    "    df = df.loc[~df['race'].isin(['Native American','Hispanic','Asian','Other']),:]\n",
    "\n",
    "    df = df[['sex','race','age_cat','c_charge_degree','score_text','priors_count','is_recid', 'two_year_recid','length_of_stay']]\n",
    "\n",
    "    df['priors_count'] = df['priors_count'].apply(lambda x: 0 if x <= 0 else ('1 to 3' if 1 <= x <= 3 else 'More than 3'))\n",
    "    df['length_of_stay'] = df['length_of_stay'].apply(lambda x: '<week' if x <= 7 else ('<3months' if 8 < x <= 93 else '>3months'))\n",
    "    df['score_text'] = df['score_text'].apply(lambda x: 'MediumHigh' if (x == 'High')| (x == 'Medium') else x)\n",
    "    df['age_cat'] = df['age_cat'].apply(lambda x: '25 to 45' if x == '25 - 45' else x)\n",
    "\n",
    "    df['sex'] = df['sex'].replace({'Female': 1.0, 'Male': 0.0})\n",
    "    df['race'] = df['race'].apply(lambda x: 1.0 if x == 'Caucasian' else 0.0)\n",
    "\n",
    "    df = df[['two_year_recid', 'sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree']]\n",
    "\n",
    "    protected_attributes = ['sex', 'race']\n",
    "    label_name = 'two_year_recid'\n",
    "    categorical_features = ['age_cat', 'priors_count', 'c_charge_degree']\n",
    "    features = categorical_features + [label_name] + protected_attributes\n",
    "\n",
    "    # privileged classes\n",
    "    privileged_classes = {\"sex\": [1.0], \"race\": [1.0]}\n",
    "\n",
    "    # protected attribute maps\n",
    "    protected_attribute_map = {\"sex\": {0.0: 'Male', 1.0: 'Female'},\n",
    "                                \"race\": {1.0: 'Caucasian', 0.0: 'Not Caucasian'}}\n",
    "\n",
    "\n",
    "    data = StandardDataset(df, label_name, favorable_classes=[0],\n",
    "                           protected_attribute_names=protected_attributes,\n",
    "                           privileged_classes=[privileged_classes[x] for x in protected_attributes],\n",
    "                           categorical_features=categorical_features,\n",
    "                           features_to_keep=features,\n",
    "                           metadata={'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "                                     'protected_attribute_maps': [protected_attribute_map[x] for x in protected_attributes]})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Reweighing ##############\n",
    "\n",
    "def reweighing_data(train, unprivileged_group, privileged_group):\n",
    "    RW = Reweighing(unprivileged_groups=unprivileged_group, privileged_groups=privileged_group)\n",
    "    RW.fit(train)\n",
    "    train_transformed = RW.transform(train)\n",
    "    \n",
    "    train_set = train_transformed\n",
    "\n",
    "    # change weights to whole numbers\n",
    "    for i in range(train_transformed.instance_weights.size):\n",
    "        train_transformed.instance_weights[i] = (round(train_transformed.instance_weights[i] / 0.1) * 0.1) * 10\n",
    "        weights = copy.deepcopy(train_transformed.instance_weights)\n",
    "\n",
    "    # change train_transformed.features and train_transformed.labels and train_transformed.protected_attributes according to the weights of each instance\n",
    "    for i in range(train_transformed.features.shape[0]):\n",
    "        row = copy.deepcopy(train_transformed.features[i])\n",
    "        row_label = copy.deepcopy(train_transformed.labels[i])\n",
    "        row_protected_attributes = copy.deepcopy(train_transformed.protected_attributes[i])\n",
    "        row_protected_attributes.resize(1,2)\n",
    "        row.resize(1,train_transformed.features.shape[1])\n",
    "        row_label.resize(1,1)\n",
    "        weight = int(weights[i])\n",
    "        for j in range(weight-1):\n",
    "            train_transformed.features = np.concatenate((train_transformed.features,row))\n",
    "            train_transformed.labels = np.concatenate((train_transformed.labels,row_label))\n",
    "            train_transformed.protected_attributes = np.concatenate((train_transformed.protected_attributes,row_protected_attributes))\n",
    "\n",
    "    # change the train_transformed to a numpy array of ones to match number of rows in features\n",
    "    train_transformed.instance_weights = np.ones(train_transformed.features.shape[0])\n",
    "\n",
    "    print(\"reweighing complete\\n\")\n",
    "\n",
    "    return train_set, train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(train, test, unprivileged_groups, privileged_groups):\n",
    "    \n",
    "    ################## adversarial debiasing #################\n",
    "\n",
    "    sess = tf.Session()\n",
    "    debiased_model_reweighing = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                                     unprivileged_groups = unprivileged_groups,\n",
    "                                                     scope_name='debiased_classifier', debias=True, sess=sess)\n",
    "    debiased_model_reweighing.fit(train)\n",
    "    dataset_debiasing_test_reweighing = debiased_model_reweighing.predict(test)\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print(\"Adversarial Debiasing\")\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_debiasing_test_reweighing,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_debiasing_test_reweighing,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "\n",
    "    accuracy_adversarial = accuracy_score(y_true = test.labels, y_pred = dataset_debiasing_test_reweighing.labels)\n",
    "\n",
    "    metrics_adversarial = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### prejudice remover #####################\n",
    "    prejudice_model_reweighing = PrejudiceRemover(eta=100, sensitive_attr='sex')\n",
    "    prejudice_model_reweighing.fit(train)\n",
    "    dataset_prejudice_test_reweighing = prejudice_model_reweighing.predict(test)\n",
    "\n",
    "    print(\"Prejudice Remover\")\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_prejudice_test_reweighing,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_prejudice_test_reweighing,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_prejudice = accuracy_score(y_true=test.labels, y_pred=dataset_prejudice_test_reweighing.labels)\n",
    "\n",
    "    metrics_prejudice = [metric_test.mean_difference(), acc_test.disparate_impact(), 0.0, 0.0, acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### normal neural net #####################\n",
    "    sess = tf.Session()\n",
    "    neural_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                        unprivileged_groups = unprivileged_groups,\n",
    "                                        scope_name='debiased_classifier', debias=False, sess=sess)\n",
    "    neural_model.fit(train)\n",
    "    dataset_neural_test = neural_model.predict(test)\n",
    "    sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print(\"Non-debiasing\")\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_neural_test,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_neural_test,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_nondebiasing = accuracy_score(y_true=test.labels, y_pred=dataset_neural_test.labels)\n",
    "\n",
    "    metrics_nondebiasing = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "\n",
    "    ##################### ensemble #####################\n",
    "    pred_labels_test = []\n",
    "    for i in range(0, len(test.features)):\n",
    "        arr_test = mode([dataset_debiasing_test_reweighing.labels[i], dataset_prejudice_test_reweighing.labels[i], dataset_neural_test.labels[i]])\n",
    "        pred_labels_test.append(arr_test[0][0])\n",
    "        dataset_ensemble_test = test.copy()\n",
    "        dataset_ensemble_test.labels = np.array(pred_labels_test)\n",
    "\n",
    "    print(\"Ensemble\")\n",
    "\n",
    "    ##################### metrics #####################\n",
    "\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_ensemble_test,\n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "    acc_test = ClassificationMetric(test, dataset_ensemble_test,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "    accuracy_ensemble = accuracy_score(y_true=test.labels, y_pred=dataset_ensemble_test.labels)\n",
    "    metrics_ensemble = [metric_test.mean_difference(),acc_test.disparate_impact(), acc_test.equal_opportunity_difference(), acc_test.average_odds_difference(), acc_test.theil_index()]\n",
    "\n",
    "    accuracy_scores = [accuracy_adversarial, accuracy_prejudice, accuracy_nondebiasing, accuracy_ensemble]\n",
    "    fairness_metrics = [metrics_adversarial, metrics_prejudice, metrics_nondebiasing, metrics_ensemble]\n",
    "    print(\"compiling all metrics complete\\n\")\n",
    "    \n",
    "    return accuracy_scores, fairness_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data complete\n",
      "reweighing complete\n",
      "\n",
      "  With reweighing\n",
      "Adversarial Debiasing\n",
      "Prejudice Remover\n",
      "Non-debiasing\n",
      "Ensemble\n",
      "compiling all metrics complete\n",
      "\n",
      "  Without reweighing\n",
      "Adversarial Debiasing\n",
      "Prejudice Remover\n",
      "Non-debiasing\n",
      "Ensemble\n",
      "compiling all metrics complete\n",
      "\n",
      "prediction completed\n"
     ]
    }
   ],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "df = pd.read_csv('dataset/compas-scores-two-years.csv')\n",
    "dataset_orig = preprocess_compasdataset(df)\n",
    "\n",
    "train, test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "print(\"splitting data complete\")\n",
    "\n",
    "train_transformed, dataset_transf_train = reweighing_data(train, unprivileged_groups, privileged_groups)\n",
    "\n",
    "print('  With reweighing')\n",
    "accuracy_reweigh, fairness_metrics_reweigh =  make_prediction(dataset_transf_train, test, unprivileged_groups, privileged_groups)\n",
    "metrics_adversarial_reweigh = fairness_metrics_reweigh[0]\n",
    "metrics_prejudice_reweigh = fairness_metrics_reweigh[1]\n",
    "metrics_nondebiasing_reweigh = fairness_metrics_reweigh[2]\n",
    "metrics_ensemble_reweigh = fairness_metrics_reweigh[3]\n",
    "\n",
    "print('  Without reweighing')\n",
    "accuracy, fairness_metrics =  make_prediction(train_transformed, test, unprivileged_groups, privileged_groups)\n",
    "metrics_adversarial = fairness_metrics[0]\n",
    "metrics_prejudice = fairness_metrics[1]\n",
    "metrics_nondebiasing = fairness_metrics[2]\n",
    "metrics_ensemble = fairness_metrics[3]\n",
    "\n",
    "print('prediction completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics with reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.649621   0.636995     0.652146  0.652146\n",
      "Mean difference                 -0.134439   0.046883    -0.115147 -0.115147\n",
      "Disparate impact                 0.806432   1.083317     0.829473  0.829473\n",
      "Equal Opportunity Difference    -0.041353   0.000000    -0.035828 -0.035828\n",
      "Average Odds Difference         -0.119742   0.000000    -0.097749 -0.097749\n",
      "Theil Index                      0.202573   0.203122     0.202986  0.202986\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Accuracy scores and Fairness metrics without reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Adversarial  Prejudice  Nondebiased  Ensemble\n",
      "Accuracy                         0.648990   0.636995     0.645202  0.644571\n",
      "Mean difference                 -0.158791   0.046883    -0.102212 -0.137582\n",
      "Disparate impact                 0.771370   1.083317     0.844937  0.801908\n",
      "Equal Opportunity Difference    -0.066710   0.000000    -0.023238 -0.050862\n",
      "Average Odds Difference         -0.144103   0.000000    -0.086050 -0.122939\n",
      "Theil Index                      0.213597   0.203122     0.210578  0.207207\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics with reweighing\"))\n",
    "\n",
    "columns = ['Adversarial', 'Prejudice', 'Nondebiased', 'Ensemble']\n",
    "index = ['Accuracy', 'Mean difference', 'Disparate impact', 'Equal Opportunity Difference', 'Average Odds Difference', 'Theil Index']\n",
    "\n",
    "mean_difference = [metrics_adversarial_reweigh[0], metrics_prejudice_reweigh[0], metrics_nondebiasing_reweigh[0], metrics_ensemble_reweigh[0]]\n",
    "disparate_impact = [metrics_adversarial_reweigh[1], metrics_prejudice_reweigh[1], metrics_nondebiasing_reweigh[1], metrics_ensemble_reweigh[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial_reweigh[2], metrics_prejudice_reweigh[2], metrics_nondebiasing_reweigh[2], metrics_ensemble_reweigh[2]]\n",
    "average_odds_difference = [metrics_adversarial_reweigh[3], metrics_prejudice_reweigh[3], metrics_nondebiasing_reweigh[3], metrics_ensemble_reweigh[3]]\n",
    "theil_index = [metrics_adversarial_reweigh[4], metrics_prejudice_reweigh[4], metrics_nondebiasing_reweigh[4], metrics_ensemble_reweigh[4]]\n",
    "\n",
    "data_reweighted = [accuracy_reweigh, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "df = pd.DataFrame(data_reweighted, index = index, columns=columns)\n",
    "print(df)\n",
    "\n",
    "display(Markdown(\"#### Accuracy scores and Fairness metrics without reweighing\"))\n",
    "\n",
    "mean_difference = [metrics_adversarial[0], metrics_prejudice[0], metrics_nondebiasing[0], metrics_ensemble[0]]\n",
    "disparate_impact = [metrics_adversarial[1], metrics_prejudice[1], metrics_nondebiasing[1], metrics_ensemble[1]]\n",
    "equal_opportunity_difference = [metrics_adversarial[2], metrics_prejudice[2], metrics_nondebiasing[2], metrics_ensemble[2]]\n",
    "average_odds_difference = [metrics_adversarial[3], metrics_prejudice[3], metrics_nondebiasing[3], metrics_ensemble[3]]\n",
    "theil_index = [metrics_adversarial[4], metrics_prejudice[4], metrics_nondebiasing[4], metrics_ensemble[4]]\n",
    "\n",
    "data = [accuracy, mean_difference, disparate_impact, equal_opportunity_difference, average_odds_difference, theil_index]\n",
    "df = pd.DataFrame(data, index = index, columns=columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
